{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import math\n",
    "import heapq\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise variables\n",
    "\n",
    "# Environment variables\n",
    "global ENV_ARRAY, CANVAS_WIDTH, CANVAS_HEIGHT, PROG_LABEL\n",
    "ENV_ARRAY = [[1,1,1,1,1,1,1],\n",
    "             [1,0,0,0,0,0,1],\n",
    "             [1,0,1,1,3,0,1],\n",
    "             [1,0,3,0,0,0,1],\n",
    "             [1,0,0,0,1,4,1],\n",
    "             [1,5,0,0,1,0,1],\n",
    "             [1,1,1,1,1,1,1]]\n",
    "CANVAS_WIDTH = 525\n",
    "CANVAS_HEIGHT = 525\n",
    "\n",
    "# Bin variables\n",
    "global AREA_ID, BIN_COLOUR, BIN_RATE, BIN_MAX, BIN_START, BINS_LOC, BINS_LOC_RATE, NOT_COLLECTED_BINS\n",
    "AREA_ID = {3:\"busy\", 4:\"moderate\", 5:\"quiet\"}\n",
    "BIN_COLOUR = {\"busy\":\"red\", \"moderate\":\"#FF8D29\", \"quiet\":\"#FFCD38\"}\n",
    "BIN_RATE = {\"busy\": 8, \"moderate\": 5, \"quiet\": 3}\n",
    "BIN_MAX = 50\n",
    "BIN_START = BIN_MAX // 2\n",
    "BINS_LOC = []\n",
    "BINS_LOC_RATE = []\n",
    "NOT_COLLECTED_BINS = []\n",
    "\n",
    "# Truck variables\n",
    "global TRUCK_START, CUR_TRUCK_START, TRUCK_MAX_CAP, TRUCK_MAX_MOVE, TRUCK_REMAIN_MOVE, AGENT_MOVE_LABEL\n",
    "TRUCK_START = [(1,1),(3,3),(5,5),(1,5)]\n",
    "CUR_TRUCK_START = TRUCK_START[0]\n",
    "TRUCK_MAX_CAP = 100\n",
    "TRUCK_MAX_MOVE = 150\n",
    "TRUCK_REMAIN_MOVE = TRUCK_MAX_MOVE\n",
    "\n",
    "# Landfill variables\n",
    "global LANDFILL_ROW, LANDFILL_COL\n",
    "LANDFILL_ROW = 1\n",
    "LANDFILL_COL = 3\n",
    "ENV_ARRAY[LANDFILL_ROW][LANDFILL_COL] = 2\n",
    "\n",
    "# # Capacity variables\n",
    "CAP_LVL = [\"empty\", \"half\", \"full\"]\n",
    "# # empty = 0-40%\n",
    "# # half = 40-70%\n",
    "# # full = 70-100%\n",
    "\n",
    "# Experiment parameters\n",
    "global EXP_NUM, TOTAL_EXP, window\n",
    "EXP_NUM = 0\n",
    "TOTAL_EXP = 4\n",
    "\n",
    "# Algorithm variables\n",
    "global ALGO_LIST, ALGO_NAME, ALGO_LABEL\n",
    "ALGO_LIST = [\"A* search algorithm\", \"Reinforcement learning: Q-Learning\"]\n",
    "ALGO_NAME = ALGO_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section was referenced and adapted from 'A* Search Algorithm' by GeeksforGeeks\n",
    "# Available at https://www.geeksforgeeks.org/a-search-algorithm/\n",
    "\n",
    "# Heuristic function for A*\n",
    "def heuristic(start,end):\n",
    "    # Euclidean distance\n",
    "    # Direct distance from start to end\n",
    "    x1, y1 = start\n",
    "    x2, y2 = end\n",
    "    return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "\n",
    "# Find possible moves from a point\n",
    "def get_neighbours(point):\n",
    "    neighbours = []\n",
    "    i, j = point\n",
    "\n",
    "    # Check top grid\n",
    "    if i > 0 and ENV_ARRAY[i-1][j] != 1:\n",
    "        neighbours.append((i-1,j))\n",
    "    \n",
    "    # Check bottom grid\n",
    "    if i < len(ENV_ARRAY)-1 and ENV_ARRAY[i+1][j] != 1:\n",
    "        neighbours.append((i+1,j))\n",
    "\n",
    "    # Check left grid\n",
    "    if j > 0 and ENV_ARRAY[i][j-1] != 1:\n",
    "        neighbours.append((i,j-1))\n",
    "    \n",
    "    # Check right grid\n",
    "    if j < len(ENV_ARRAY[0])-1 and ENV_ARRAY[i][j+1] != 1:\n",
    "        neighbours.append((i,j+1))\n",
    "\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "# A* algorithm\n",
    "def a_star(start,end):\n",
    "    # Initialise start node scores\n",
    "    gscore = {start: 0}\n",
    "    fscore = {start: heuristic(start,end)}\n",
    "\n",
    "    # Initialise open and closed list\n",
    "    open_list = [(fscore[start],start)]\n",
    "    closed_list = []\n",
    "\n",
    "    # Initialise previous node for path tracking\n",
    "    previous = {}\n",
    "\n",
    "    while open_list:\n",
    "        # Find node with smallest fscore\n",
    "        # Pop node from open list\n",
    "        current = heapq.heappop(open_list)[1]\n",
    "\n",
    "        # Reach end node\n",
    "        # Reconstruct path\n",
    "        if current == end:\n",
    "            path = []\n",
    "            while current in previous:\n",
    "                path.append(current)\n",
    "                current = previous[current]\n",
    "                if current == start:\n",
    "                    break\n",
    "            path.reverse()\n",
    "            return path\n",
    "\n",
    "        # Add current node to closed list\n",
    "        closed_list.append(current)\n",
    "            \n",
    "        # Find neigbours\n",
    "        for neighbour in get_neighbours(current):\n",
    "            # Skip nodes in closed list\n",
    "            if neighbour in closed_list:\n",
    "                continue\n",
    "            \n",
    "            # Compute scores\n",
    "            temp_gscore = gscore[current] + 1\n",
    "            temp_fscore = temp_gscore + heuristic(neighbour,end)\n",
    "\n",
    "            # Add new node to open list\n",
    "            if neighbour not in [n for _,n in open_list if n == neighbour]:\n",
    "                gscore[neighbour] = temp_gscore\n",
    "                fscore[neighbour] = temp_fscore\n",
    "                previous[neighbour] = current\n",
    "                open_list.append((temp_fscore,neighbour))\n",
    "\n",
    "            # If neighbour is in open list\n",
    "            # If score improves\n",
    "            elif temp_fscore < fscore.get(neighbour):\n",
    "                # Update node details\n",
    "                gscore[neighbour] = temp_gscore\n",
    "                fscore[neighbour] = temp_fscore\n",
    "                previous[neighbour] = current\n",
    "    \n",
    "    # No path found\n",
    "    # Should never reach this point\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_bins():\n",
    "    global NOT_COLLECTED_BINS\n",
    "    NOT_COLLECTED_BINS = BINS_LOC[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest next bin\n",
    "def next_bin(start):\n",
    "    nearest_bin = None\n",
    "    nearest_distance = float(\"inf\")\n",
    "    nearest_path = None\n",
    "    \n",
    "    # Compute A* path distance\n",
    "    for bin in NOT_COLLECTED_BINS:\n",
    "        path = a_star(start,bin)\n",
    "\n",
    "        # No path found\n",
    "        # Should never reach here\n",
    "        if path is None:\n",
    "            print(\"No path found to\", bin)\n",
    "            continue\n",
    "        \n",
    "        # Calculate distance to bin\n",
    "        distance = len(path)\n",
    "        \n",
    "        # Update nearest bin, distance, and path\n",
    "        if distance < nearest_distance:\n",
    "            nearest_bin = bin\n",
    "            nearest_distance = distance\n",
    "            nearest_path = path\n",
    "    \n",
    "    # Update agent destination\n",
    "    x, y = nearest_bin\n",
    "    prog_text = \"Move to bin ({}, {})\"\n",
    "    PROG_LABEL.config(text=prog_text.format(x,y))\n",
    "    NOT_COLLECTED_BINS.remove(nearest_bin)\n",
    "\n",
    "    return nearest_path\n",
    "\n",
    "\n",
    "# Find nearest path to landfill\n",
    "def to_landfill(start):\n",
    "    path = a_star(start,(LANDFILL_ROW,LANDFILL_COL))\n",
    "    reset_bins()\n",
    "\n",
    "    # Update agent destination\n",
    "    prog_text = \"Move to landfill\"\n",
    "    PROG_LABEL.config(text=prog_text)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement learning variable initialisation\n",
    "def rf_init():\n",
    "    # Initialise states\n",
    "    global states\n",
    "    states = [((x,y),truck_fill) \\\n",
    "              for x in range(len(ENV_ARRAY)) \\\n",
    "              for y in range(len(ENV_ARRAY[0])) \\\n",
    "              for truck_fill in range(len(CAP_LVL))]\n",
    "    \n",
    "    # Initialise actions\n",
    "    global actions\n",
    "    actions = [\"move_up\",\"move_down\",\"move_left\",\"move_right\"]\n",
    "    \n",
    "    # Initialise q-table\n",
    "    global q_table\n",
    "    num_states = len(states)\n",
    "    num_actions = len(actions)\n",
    "    q_table = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    # Initialise hyperparameter\n",
    "    global learning_rate, discount_factor, epsilon\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 0.9\n",
    "    epsilon = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was adapted from 'Q-Learning: A Complete Example in Python' by Daniel Soper\n",
    "# Available at https://colab.research.google.com/drive/1E2RViy7xmor0mhqskZV14_NUj2jMpJz3\n",
    "\n",
    "# Check if reached terminal state (no more moves and all grid spaces are explored)\n",
    "def is_terminal():\n",
    "    if TIME_LAPSE == TRUCK_MAX_MOVE * 5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_reward(next_state,collect_amount,empty_amount):\n",
    "    # Reward for collecting trash\n",
    "    if collect_amount < 20:\n",
    "        collect_reward = collect_amount * 2\n",
    "    else:\n",
    "        collect_reward = collect_amount * 3\n",
    "\n",
    "    # Reward for emptying at landfill\n",
    "    if empty_amount < 40:\n",
    "        empty_reward = empty_amount * 1.5\n",
    "    elif empty_amount < 70:\n",
    "        empty_reward = empty_amount * 3\n",
    "    else:\n",
    "        empty_reward = empty_amount * 4\n",
    "\n",
    "    # Penalty for collisions\n",
    "    collision_penalty = 0\n",
    "    row = next_state[0][0]\n",
    "    col = next_state[0][1]\n",
    "    if ENV_ARRAY[row][col] == 1:\n",
    "        collision_penalty -= 1500\n",
    "\n",
    "    # Penalty for each step\n",
    "    movement_penalty = -10\n",
    "\n",
    "    # Penalty for uncollected bin\n",
    "    uncollected_penalty = 0\n",
    "    for bin in bins_exact_fill:\n",
    "        if bin > 0:\n",
    "            uncollected_penalty -= 40\n",
    "            uncollected_penalty -= bin\n",
    "\n",
    "    # Total reward\n",
    "    reward = collect_reward + uncollected_penalty + \\\n",
    "             empty_reward + collision_penalty + movement_penalty\n",
    "    return reward\n",
    "\n",
    "\n",
    "# Update q-value\n",
    "# Use TD error and Bellman equation\n",
    "def update_q_value(state,action,reward,next_state):\n",
    "    current_q = q_table[states.index(state)][action]\n",
    "    next_max_q = np.max(q_table[states.index(next_state)])\n",
    "    td_error = reward + discount_factor * next_max_q - current_q\n",
    "    new_q_value = current_q + learning_rate * td_error \n",
    "    q_table[states.index(state)][action] = new_q_value\n",
    "\n",
    "# Epsilon-greedy exploration\n",
    "def select_action(state):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        q_values = q_table[states.index(state)]\n",
    "        max_q_value = np.max(q_values)\n",
    "        max_indices = np.where(q_values == max_q_value)[0]\n",
    "        if len(max_indices) > 1:\n",
    "            action = np.random.choice(max_indices)\n",
    "        else:\n",
    "            action = max_indices[0]\n",
    "    else:\n",
    "        action = np.random.randint(len(actions))\n",
    "    return action\n",
    "\n",
    "# Determine next state and reward\n",
    "def take_action(state,action):\n",
    "    global truck_exact_fill, bins_exact_fill, TIME_LAPSE\n",
    "\n",
    "    # Extract info from state\n",
    "    (truck_x,truck_y), truck_fill = state\n",
    "    new_truck_x = truck_x\n",
    "    new_truck_y = truck_y\n",
    "    new_truck_fill = truck_fill\n",
    "\n",
    "    # Update truck location\n",
    "    if action == 0 and truck_x > 0: # Move up\n",
    "        new_truck_x -= 1\n",
    "    elif action == 1 and truck_x < len(ENV_ARRAY)-1: # Move down\n",
    "        new_truck_x += 1\n",
    "    elif action == 2 and truck_y > 0: # Move left\n",
    "        new_truck_y -= 1\n",
    "    elif action == 3 and truck_y < len(ENV_ARRAY[0])-1: # Move right\n",
    "        new_truck_y += 1\n",
    "\n",
    "    # Update bin exact fill\n",
    "    TIME_LAPSE += 1\n",
    "    if TIME_LAPSE % 5 == 0:\n",
    "        for i in range(len(bins_exact_fill)):\n",
    "            bins_exact_fill[i] += BINS_LOC_RATE[i]\n",
    "    \n",
    "    # Collect and empty truck fill\n",
    "    collect_amount = 0\n",
    "    empty_amount = 0\n",
    "    if (new_truck_x,new_truck_y) == (LANDFILL_ROW,LANDFILL_COL): # At landfill\n",
    "        empty_amount = truck_exact_fill\n",
    "        truck_exact_fill = 0\n",
    "    elif (new_truck_x,new_truck_y) in BINS_LOC: # At bin\n",
    "        truck_can_collect = TRUCK_MAX_CAP - truck_exact_fill\n",
    "        bin_index = BINS_LOC.index((new_truck_x,new_truck_y))\n",
    "        trash = bins_exact_fill[bin_index]\n",
    "        if truck_can_collect >= trash: # Collect all\n",
    "            collect_amount = bins_exact_fill[bin_index]\n",
    "            bins_exact_fill[bin_index] = 0\n",
    "            truck_exact_fill += trash\n",
    "        else: # Collect partial\n",
    "            collect_amount = truck_can_collect\n",
    "            bins_exact_fill[bin_index] -= truck_can_collect\n",
    "            truck_exact_fill = TRUCK_MAX_CAP\n",
    "\n",
    "    # Update truck fill\n",
    "    truck_fill_level = truck_exact_fill / TRUCK_MAX_CAP\n",
    "    if truck_fill_level < 0.4:\n",
    "        new_truck_fill = 0\n",
    "    elif truck_fill_level < 0.7:\n",
    "        new_truck_fill = 1\n",
    "    else:\n",
    "        new_truck_fill = 2\n",
    "\n",
    "    next_state = ((new_truck_x,new_truck_y),new_truck_fill)\n",
    "    \n",
    "    # Calculate reward based on next state\n",
    "    reward = get_reward(next_state,collect_amount,empty_amount)\n",
    "\n",
    "    return next_state,reward\n",
    "        \n",
    "\n",
    "def q_learning(env):\n",
    "    for episode in range(1000):\n",
    "        # Initialise variables\n",
    "        global TIME_LAPSE, truck_exact_fill, bins_exact_fill\n",
    "        TIME_LAPSE = 0\n",
    "        truck_exact_fill = 0\n",
    "        bins_exact_fill = [BIN_START] * len(BINS_LOC)\n",
    "\n",
    "        # Initalise environment and state\n",
    "        env.reset()\n",
    "        state = env.get_state()\n",
    "\n",
    "        while not is_terminal():\n",
    "            # Choose action\n",
    "            action = select_action(state)\n",
    "\n",
    "            # Observe next state and reward\n",
    "            next_state, reward = take_action(state,action)\n",
    "            \n",
    "            # Update q-value for current state and action\n",
    "            update_q_value(state,action,reward,next_state)\n",
    "\n",
    "            # Update state for next step\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(canvas,env,start=None,to_bin=None,state=None):\n",
    "   # A* search\n",
    "   if ALGO_NAME == ALGO_LIST[0]:\n",
    "        # All bins collected\n",
    "        if not NOT_COLLECTED_BINS:\n",
    "            to_bin = False\n",
    "\n",
    "        if to_bin:\n",
    "            path = next_bin(start)\n",
    "        else:\n",
    "            path = to_landfill(start)\n",
    "        canvas.after(300,env.update_A_star,path)\n",
    "   \n",
    "   # Reinforcement learning: Q-learning\n",
    "   else:\n",
    "        q_values = q_table[states.index(state)]\n",
    "        max_q_value = np.max(q_values)\n",
    "        max_indices = np.where(q_values == max_q_value)[0]\n",
    "        if len(max_indices) > 1:\n",
    "            action = np.random.choice(max_indices)\n",
    "        else:\n",
    "            action = max_indices[0]\n",
    "       \n",
    "        if action == 0: # Move up\n",
    "           path = (state[0][0]-1,state[0][1])\n",
    "        elif action == 1: # Move down\n",
    "           path = (state[0][0]+1,state[0][1])\n",
    "        elif action == 2: # Move left\n",
    "           path = (state[0][0],state[0][1]-1)\n",
    "        elif action == 3: # Move right\n",
    "            path = (state[0][0],state[0][1]+1)\n",
    "\n",
    "        prog_text = \"\"\n",
    "        PROG_LABEL.config(text=prog_text)\n",
    "       \n",
    "        canvas.after(300,env.update_RL,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_run(landfill,truck):\n",
    "    global EXP_NUM, ALGO_NAME,CUR_TRUCK_START\n",
    "    EXP_NUM += 1\n",
    "\n",
    "    if EXP_NUM == 1:\n",
    "        print(\"\\033[1m\" + ALGO_NAME + \"\\033[0m\")\n",
    "    print(\"\\033[3mExperiment\",EXP_NUM,\"\\033[0m\")\n",
    "    print(\"Total amount of garbage collected:\",landfill+truck)\n",
    "    print(\"In landfill:\", landfill)\n",
    "    print(\"On agent:\",truck)\n",
    "    print()\n",
    "\n",
    "    # End of experiment\n",
    "    if ALGO_NAME == ALGO_LIST[0]: # First algorithm\n",
    "        if EXP_NUM == TOTAL_EXP:\n",
    "            prog_text = \"End of experiment 1\"\n",
    "            PROG_LABEL.config(text=prog_text)\n",
    "            \n",
    "            # Reset for next algorithm\n",
    "            EXP_NUM = 0\n",
    "            ALGO_NAME = ALGO_LIST[1]\n",
    "            CUR_TRUCK_START = TRUCK_START[0]\n",
    "            ALGO_LABEL.config(text=ALGO_NAME)\n",
    "            return False, True\n",
    "        else:\n",
    "            # Change truck start position\n",
    "            CUR_TRUCK_START = TRUCK_START[EXP_NUM]\n",
    "    \n",
    "    if ALGO_NAME == ALGO_LIST[1]: # Second algorithm\n",
    "        if EXP_NUM == TOTAL_EXP: # Last experiment\n",
    "            prog_text = \"End of experiment 2\"\n",
    "            PROG_LABEL.config(text=prog_text)\n",
    "            return False, False\n",
    "        else:\n",
    "            # Change truck start position\n",
    "            CUR_TRUCK_START = TRUCK_START[EXP_NUM]\n",
    "    \n",
    "    return True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was adapted from the lab 1 simple bot code\n",
    "# Specifically the Bot class\n",
    "\n",
    "class Truck():\n",
    "    def __init__(self,size,canvas):\n",
    "        self.x, self.y = CUR_TRUCK_START\n",
    "        self.cell_size = size\n",
    "        self.centre = self.cell_size/2\n",
    "        self.size = self.cell_size/2*0.2\n",
    "        self.direction = \"down\"\n",
    "        self.capacity = 0\n",
    "        self.name = \"agent\"\n",
    "        self.canvas = canvas\n",
    "\n",
    "    def draw(self):\n",
    "        if self.direction == \"up\":\n",
    "            head_x = self.x * self.cell_size + self.size\n",
    "            head_y = self.y * self.cell_size + self.centre\n",
    "            x1 = (self.x+1) * self.cell_size - self.size\n",
    "            y1 = self.y * self.cell_size + self.size\n",
    "            x2 = x1\n",
    "            y2 = y1 + self.cell_size - 2*self.size\n",
    "        elif self.direction == \"down\":\n",
    "            head_x = (self.x+1) * self.cell_size - self.size\n",
    "            head_y = self.y * self.cell_size + self.centre\n",
    "            x1 = self.x * self.cell_size + self.size\n",
    "            y1 = self.y * self.cell_size + self.size\n",
    "            x2 = x1\n",
    "            y2 = y1 + self.cell_size - 2*self.size\n",
    "        elif self.direction == \"right\":\n",
    "            head_x = self.x * self.cell_size + self.centre\n",
    "            head_y = (self.y+1) * self.cell_size - self.size\n",
    "            x1 = self.x * self.cell_size + self.size\n",
    "            y1 = self.y * self.cell_size + self.size\n",
    "            x2 = x1 + self.cell_size - 2*self.size\n",
    "            y2 = y1\n",
    "        else:\n",
    "            head_x = self.x * self.cell_size + self.centre\n",
    "            head_y = self.y * self.cell_size + self.size\n",
    "            x1 = self.x * self.cell_size + self.size\n",
    "            y1 = (self.y+1) * self.cell_size - self.size\n",
    "            x2 = x1 + self.cell_size - 2*self.size\n",
    "            y2 = y1\n",
    "        \n",
    "        vertices = [head_y,head_x,y1,x1,y2,x2]\n",
    "        self.canvas.delete(self.name)\n",
    "        self.canvas.create_polygon(vertices,fill=\"blue\", tags=self.name)\n",
    "\n",
    "        # Update truck capacity\n",
    "        self.update_capacity()\n",
    "\n",
    "    def move(self,x,y):\n",
    "        global TRUCK_REMAIN_MOVE\n",
    "        # Stop truck if reached max steps\n",
    "        if TRUCK_REMAIN_MOVE == 0:\n",
    "            PROG_LABEL.config(text=\"No more moves\")\n",
    "            return False\n",
    "        \n",
    "        TRUCK_REMAIN_MOVE -= 1\n",
    "        label_text = f\"Remaining {TRUCK_REMAIN_MOVE} moves\"\n",
    "        AGENT_MOVE_LABEL.config(text=label_text)\n",
    "\n",
    "        if self.x < x:\n",
    "            self.direction = \"down\"\n",
    "        elif self.x > x:\n",
    "            self.direction = \"up\"\n",
    "        elif self.y < y:\n",
    "            self.direction = \"right\"\n",
    "        elif self.y > y:\n",
    "            self.direction = \"left\"\n",
    "\n",
    "        if x > 0 and y > 0 and x < len(ENV_ARRAY) and y < len(ENV_ARRAY):\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            self.draw()\n",
    "        return True\n",
    "\n",
    "    def update_capacity(self):\n",
    "        x = self.x * self.cell_size + self.centre\n",
    "        y = self.y * self.cell_size + self.centre\n",
    "        self.capacity_tag = self.canvas.create_text(y,x, \\\n",
    "                                                    text=str(self.capacity), \\\n",
    "                                                    fill=\"white\", \\\n",
    "                                                    font=(\"Arial\",14), \\\n",
    "                                                    tags=(self.name,\"capacity\"))\n",
    "    \n",
    "    def collect(self,bin):\n",
    "        if TRUCK_MAX_CAP < self.capacity + bin:\n",
    "            collect = TRUCK_MAX_CAP - self.capacity\n",
    "            uncollected = bin - collect\n",
    "            self.capacity = TRUCK_MAX_CAP\n",
    "        else:\n",
    "            self.capacity += bin\n",
    "            uncollected = 0\n",
    "        return uncollected\n",
    "    \n",
    "    def empty(self):\n",
    "        self.capacity = 0\n",
    "\n",
    "    def getLocation(self):\n",
    "        return self.x, self.y\n",
    "    \n",
    "    def reset(self):\n",
    "        global TRUCK_REMAIN_MOVE\n",
    "        self.x, self.y = CUR_TRUCK_START\n",
    "        self.direction = \"down\"\n",
    "        self.capacity = 0\n",
    "        TRUCK_REMAIN_MOVE = TRUCK_MAX_MOVE\n",
    "        self.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was adapted from the lab 1 simple bot code\n",
    "# Specifically the Lamp class\n",
    "\n",
    "class Bin():\n",
    "    def __init__(self,row,col,size,area,canvas):\n",
    "        self.name = str((row,col))\n",
    "        self.capacity = \"capacity\" + self.name\n",
    "        self.cell_centre = size//2\n",
    "        self.x = row*size + self.cell_centre\n",
    "        self.y = col*size + self.cell_centre\n",
    "        self.radius = size//3\n",
    "        self.colour = BIN_COLOUR[area]\n",
    "        self.canvas = canvas\n",
    "        self.rate = BIN_RATE[area]\n",
    "        self.filled = BIN_START\n",
    "        if area == \"busy\":\n",
    "            self.text_colour = \"white\"\n",
    "        else:\n",
    "            self.text_colour = \"black\"\n",
    "\n",
    "    def draw(self):\n",
    "        self.canvas.create_oval(self.y-self.radius,self.x-self.radius, \\\n",
    "                                self.y+self.radius,self.x+self.radius, \\\n",
    "                                fill=self.colour,outline='', \\\n",
    "                                tags=self.name)\n",
    "        self.update_capacity()\n",
    "\n",
    "    def update_capacity(self):\n",
    "        self.capacity_tag = self.canvas.create_text(self.y,self.x, \\\n",
    "                                                    text=str(self.filled), \\\n",
    "                                                    fill=self.text_colour, \\\n",
    "                                                    font=(\"Arial\",14), \\\n",
    "                                                    tags=(self.name,self.capacity))\n",
    "    \n",
    "    def update_load(self,load):\n",
    "        self.filled = load\n",
    "        self.canvas.delete(self.capacity)\n",
    "        self.draw()\n",
    "\n",
    "    def add_load(self):\n",
    "        load = self.filled + self.rate\n",
    "        if load > BIN_MAX:\n",
    "            load = BIN_MAX\n",
    "        self.update_load(load)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.filled = 50\n",
    "        self.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was adapted from the lab 1 simple bot code\n",
    "\n",
    "class Landfill():\n",
    "    def __init__(self,canvas):\n",
    "        self.name = \"landfill\"\n",
    "        self.capacity = 0\n",
    "        self.canvas = canvas\n",
    "\n",
    "    def set_loc(self,x1,y1,x2,y2):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "        self.centre_x = (x1+x2) / 2\n",
    "        self.centre_y = (y1+y2) / 2\n",
    "    \n",
    "    def draw(self):\n",
    "        self.canvas.create_rectangle(self.y1,self.x1,self.y2,self.x2, \\\n",
    "                                    fill=\"brown\",outline=\"black\", \\\n",
    "                                    tags=self.name)\n",
    "        self.canvas.create_text(self.centre_y,self.centre_x, \\\n",
    "                                text=str(self.capacity), \\\n",
    "                                fill=\"white\", \\\n",
    "                                font=(\"Arial\",18), \\\n",
    "                                tags=self.name)\n",
    "    \n",
    "    def update_capacity(self,load):\n",
    "        self.capacity += load\n",
    "    \n",
    "    def reset(self):\n",
    "        self.capacity = 0\n",
    "        self.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self,canvas):\n",
    "        global BINS_LOC, BINS_LOC_RATE\n",
    "        self.rows = len(ENV_ARRAY)\n",
    "        self.cols = len(ENV_ARRAY[0])\n",
    "        self.cell_size = CANVAS_WIDTH // self.cols\n",
    "        self.bins = []\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                area = ENV_ARRAY[i][j]\n",
    "                if area > 2:\n",
    "                    self.bins.append(Bin(i,j,self.cell_size,AREA_ID[area],canvas))\n",
    "                    BINS_LOC.append((i,j))\n",
    "                    BINS_LOC_RATE.append(BIN_RATE[AREA_ID[area]])\n",
    "        self.truck = Truck(self.cell_size,canvas)\n",
    "        self.landfill = Landfill(canvas)\n",
    "        self.canvas = canvas\n",
    "\n",
    "    def draw(self):\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                x1 = i * self.cell_size\n",
    "                y1 = j * self.cell_size\n",
    "                x2 = x1 + self.cell_size\n",
    "                y2 = y1 + self.cell_size\n",
    "\n",
    "                if ENV_ARRAY[i][j] == 1:\n",
    "                    self.canvas.create_rectangle(y1,x1,y2,x2, \\\n",
    "                                                 fill=\"#808080\",outline=\"black\")\n",
    "                else:\n",
    "                    if ENV_ARRAY[i][j] == 2:\n",
    "                        self.landfill.set_loc(x1,y1,x2,y2)\n",
    "                        self.landfill.draw()\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.canvas.create_rectangle(y1,x1,y2,x2, \\\n",
    "                                                    fill=\"white\",outline=\"black\")\n",
    "                \n",
    "                # Add row index\n",
    "                if i == 0:\n",
    "                    x = x1 + self.cell_size / 2\n",
    "                    y = y1 + self.cell_size / 2\n",
    "                    self.canvas.create_text(y,x,text=str(j), \\\n",
    "                                            fill=\"white\", \\\n",
    "                                            font=(\"Arial\",16,\"bold\"))\n",
    "                # Add column index\n",
    "                elif j == 0:\n",
    "                    x = x1 + self.cell_size / 2\n",
    "                    y = y1 + self.cell_size / 2\n",
    "                    self.canvas.create_text(y,x,text=str(i), \\\n",
    "                                            fill=\"white\", \\\n",
    "                                            font=(\"Arial\",16,\"bold\"))\n",
    "                    \n",
    "        for bin in self.bins:\n",
    "            bin.draw()\n",
    "\n",
    "        self.truck.draw()\n",
    "    \n",
    "    def truck_at_loc(self):\n",
    "        truck_loc = self.truck.getLocation()\n",
    "        \n",
    "        # Truck at bin\n",
    "        if truck_loc in BINS_LOC:\n",
    "            index = BINS_LOC.index(truck_loc)\n",
    "            bin = self.bins[index]\n",
    "            \n",
    "            # Truck collect, empty bin\n",
    "            bin_load = bin.filled\n",
    "            remain_bin = self.truck.collect(bin_load)\n",
    "            bin.update_load(remain_bin)\n",
    "            self.truck.draw()\n",
    "\n",
    "        # Truck at landfill\n",
    "        elif truck_loc == (LANDFILL_ROW,LANDFILL_COL):\n",
    "            self.landfill.update_capacity(self.truck.capacity)\n",
    "            self.landfill.draw()\n",
    "            self.truck.empty()\n",
    "            self.truck.draw()\n",
    "            \n",
    "    def update_A_star(self,path):\n",
    "        # Move agent\n",
    "        x, y = path[0]\n",
    "        truck_move = self.truck.move(x,y)\n",
    "\n",
    "        # Max truck steps has not reached\n",
    "        if truck_move:\n",
    "            # Update bins\n",
    "            if (TRUCK_MAX_MOVE - TRUCK_REMAIN_MOVE) % 5 == 0:\n",
    "                for bin in self.bins:\n",
    "                    bin.add_load()\n",
    "\n",
    "            # Remove moved path\n",
    "            path = path[1:]\n",
    "\n",
    "            if path:\n",
    "                self.canvas.after(300,self.update_A_star,path)\n",
    "            else:\n",
    "                # Reached destination\n",
    "                self.truck_at_loc()\n",
    "                move(self.canvas,self,start=(x,y), \\\n",
    "                     to_bin=self.truck.capacity != TRUCK_MAX_CAP)\n",
    "        \n",
    "        # End experiment run\n",
    "        else:\n",
    "            next_run,next_exp = end_run(self.landfill.capacity,self.truck.capacity)\n",
    "            \n",
    "            if next_run:\n",
    "                prog_text = \"Reset environment for next run\"\n",
    "            elif next_exp:\n",
    "                prog_text = \"Reset environment for next experiment\"\n",
    "\n",
    "            # Next experiment run\n",
    "            if next_run or next_exp:\n",
    "                window.update()\n",
    "                time.sleep(1)\n",
    "                PROG_LABEL.config(text=prog_text)\n",
    "                window.update()\n",
    "                time.sleep(0.5)\n",
    "                self.reset()\n",
    "                if ALGO_NAME == ALGO_LIST[0]:\n",
    "                    move(self.canvas,self,start=CUR_TRUCK_START,to_bin=True)\n",
    "                else:\n",
    "                    rf_init()\n",
    "                    q_learning(self)\n",
    "                    state = self.get_state()\n",
    "                    move(self.canvas,self,state=state)\n",
    "\n",
    "            # End of all experiments\n",
    "            else:\n",
    "                window.update()\n",
    "                time.sleep(1)\n",
    "                prog_text = \"End of all experiments\"\n",
    "                PROG_LABEL.config(text=prog_text)\n",
    "\n",
    "\n",
    "    def update_RL(self,path):\n",
    "        # Move agent\n",
    "        x, y = path\n",
    "        truck_move = self.truck.move(x,y)\n",
    "        \n",
    "        # Max truck steps has not reached\n",
    "        if truck_move:\n",
    "            # Update bins\n",
    "            if (TRUCK_MAX_MOVE - TRUCK_REMAIN_MOVE) % 5 == 0:\n",
    "                for bin in self.bins:\n",
    "                    bin.add_load()\n",
    "            \n",
    "            # Collect / empty truck\n",
    "            self.truck_at_loc()\n",
    "\n",
    "            # Next step\n",
    "            state = self.get_state()\n",
    "            move(self.canvas,self,state=state)\n",
    "        else:\n",
    "            next_run,next_exp = end_run(self.landfill.capacity,self.truck.capacity)\n",
    "            \n",
    "            if next_run:\n",
    "                prog_text = \"Reset environment for next run\"\n",
    "                window.update()\n",
    "                time.sleep(1)\n",
    "                PROG_LABEL.config(text=prog_text)\n",
    "                window.update()\n",
    "                time.sleep(0.5)\n",
    "                self.reset()\n",
    "                rf_init()\n",
    "                q_learning(self)\n",
    "                state = self.get_state()\n",
    "                move(self.canvas,self,state=state)\n",
    "        \n",
    "    def reset(self):\n",
    "        for bin in self.bins:\n",
    "            bin.reset()\n",
    "        self.truck.reset()\n",
    "        self.landfill.reset()\n",
    "        reset_bins()\n",
    "    \n",
    "    def get_state(self):\n",
    "        truck_fill_level = self.truck.capacity / TRUCK_MAX_CAP\n",
    "        if truck_fill_level < 0.4:\n",
    "            truck_fill = 0\n",
    "        elif truck_fill_level < 0.7:\n",
    "            truck_fill = 1\n",
    "        else:\n",
    "            truck_fill = 2\n",
    "\n",
    "        state = (self.truck.getLocation(),truck_fill)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(window):\n",
    "    window.resizable(False,False)\n",
    "    window.title(\"Garbage Collecting Agent\")\n",
    "\n",
    "    # Display algorithm name\n",
    "    global ALGO_LABEL\n",
    "    ALGO_LABEL = tk.Label(window,text=ALGO_NAME,font=(\"Arial\",20,\"bold\",\"underline\"))\n",
    "    ALGO_LABEL.pack()\n",
    "\n",
    "    # Display progress\n",
    "    global PROG_LABEL\n",
    "    PROG_LABEL = tk.Label(window,text=\"\",font=(\"Arial\",20))\n",
    "    PROG_LABEL.pack()\n",
    "\n",
    "    # Environment canvas\n",
    "    canvas = tk.Canvas(window,width=CANVAS_WIDTH,height=CANVAS_HEIGHT)\n",
    "    canvas.pack()\n",
    "\n",
    "    # Display bin info label\n",
    "    tk.Label(window,text=\"Bin\",justify='left',font=(\"Arial\",14,\"bold\")) \\\n",
    "            .pack(padx=4,anchor=\"w\")\n",
    "    red_bin = f\"Red bin: busy area (increase by {BIN_RATE['busy']} every 5 steps)\\n\"\n",
    "    orange_bin = f\"Orange bin: moderately busy area (increase by {BIN_RATE['moderate']} every 5 steps)\\n\"\n",
    "    yellow_bin = f\"Yellow bin: quiet area (increase by {BIN_RATE['quiet']} every 5 steps)\\n\\n\"\n",
    "    info1 = f\"Max bin load is {BIN_MAX}\\n\"\n",
    "    info2 = \"*Bin load will increase even when truck is collecting from it*\"\n",
    "    label_text = red_bin + orange_bin + yellow_bin + info1 + info2\n",
    "    tk.Label(window,text=label_text,justify='left') \\\n",
    "            .pack(padx=4,anchor=\"w\")\n",
    "    \n",
    "    # Display agent info label\n",
    "    tk.Label(window,text=\"Agent\",justify='left',font=(\"Arial\",14,\"bold\")) \\\n",
    "            .pack(padx=4,side=\"top\",anchor=\"w\")\n",
    "    info1 = f\"Max agent load is {TRUCK_MAX_CAP}\\n\"\n",
    "    info2 = f\"Max agent steps is {TRUCK_MAX_MOVE}\"\n",
    "    label_text = info1 + info2\n",
    "    tk.Label(window,text=label_text,justify='left') \\\n",
    "            .pack(padx=4,anchor=\"w\")\n",
    "    \n",
    "    global AGENT_MOVE_LABEL\n",
    "    label_text = f\"Remaining {TRUCK_MAX_MOVE} moves\"\n",
    "    AGENT_MOVE_LABEL = tk.Label(window,text=label_text)\n",
    "    AGENT_MOVE_LABEL.pack(padx=4,anchor=\"w\")\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global window\n",
    "    window = tk.Tk()\n",
    "    canvas = initialise(window)\n",
    "    env = Environment(canvas)\n",
    "    env.draw()\n",
    "    reset_bins()\n",
    "    move(canvas,env,start=CUR_TRUCK_START,to_bin=True)\n",
    "    window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
